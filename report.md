# Итеративные методы (Зейделя)

-   **Студент** Карпич Иван Валерьевич
-   **Группа** 3823Б1ПР4
-   **Технология** SEQ|MPI
-   **Вариант** 19

## 1. Введение

Решение систем линейных уравнений является фундаментальной задачей в численных методах и вычислительной математике. Метод Зейделя является одним из итерационных методов, позволяющих найти решение системы линейных уравнений Ax = b с заданной точностью.

Преимущество метода Зейделя перед прямыми методами (например, методом исключения Гаусса) состоит в его простоте реализации и возможности параллелизации. Метод Зейделя часто используется для больших разреженных матриц, где прямые методы могут быть неэффективны.

Цель данной работы: реализация последовательного и параллельного алгоритмов решения системы линейных уравнений методом Зейделя с использованием технологии MPI, а также анализ производительности параллельной реализации.

## 2. Постановка задачи

Требуется решить систему линейных уравнений вида:

Ax = b

где:

-   A - квадратная матрица размером n × n
-   x - вектор неизвестных (искомое решение)
-   b - вектор свободных членов

Входные данные:

```cpp
using InType = std::tuple<std::size_t /* n */,
               std::vector<double>, /* a */,
               std::vector<double>, /* b */,
               double /* eps */>;
```

Выходные данные:

```cpp
using OutType = std::vector<double>;
```

Ограничения:

-   n > 0
-   Матрица A должна быть невырождена (det(A) ≠ 0)
-   eps > 0
-   Метод должен сойтись при заданной точности

## 3. Последовательный алгоритм (по коду)

Метод Зейделя (Gauss–Seidel) в моей реализации решает систему линейных уравнений итеративно, **последовательно** обновляя компоненты `x[i]` и используя **уже обновлённые значения** в рамках текущей итерации (in-place).

### Описание алгоритма

1. Инициализация

Вектор решения создаётся и заполняется нулями:

```cpp
std::vector<double> x(n, 0);
```

Также создаётся массив ошибок (разниц между новым и старым значением компоненты):

```cpp
std::vector<double> epsilons(n, -1);
```

2. Итерационный процесс (in-place)

Итерации выполняются, пока `iter_continue == true`:

```cpp
bool iter_continue = true;
while (iter_continue) {
  ...
}
```

На каждой итерации последовательно по `i` вычисляется новое значение `x[i]`:

```cpp
for (std::size_t i = 0; i < n; i++) {
  double ix = b[i];
  ...
  ix = ix / a[(i * n) + i];
  epsilons[i] = std::fabs(ix - x[i]);
  x[i] = ix;
}
```

#### Использование in-place массива `x`

- Суммирование по `j < i` использует **уже обновлённые** значения `x[j]` текущей итерации (потому что `x[j]` уже был перезаписан ранее в этом же цикле по `i`):

```cpp
for (std::size_t j = 0; j < i; j++) {
  ix = ix - (a[(i * n) + j] * x[j]);
}
```

- Суммирование по `j > i` использует **ещё не обновлённые** значения `x[j]` (они остались от предыдущего шага/итерации, т.к. `x[j]` будет обновлён позже):

```cpp
for (std::size_t j = i + 1; j < n; j++) {
  ix = ix - (a[(i * n) + j] * x[j]);
}
```


3. Проверка сходимости

После обновления всех компонент считается максимум по `epsilons` и принимается решение — продолжать или остановиться.

Вызов проверки:

```cpp
iter_continue = IterContinue(epsilons, eps);
```

Реализация критерия:

```cpp
bool KarpichISeidolMethodSEQ::IterContinue(std::vector<double> &iter_eps, double correct_eps) {
  double max_in_iter = *std::ranges::max_element(iter_eps);
  return max_in_iter > correct_eps;
}
```

То есть итерации продолжаются, пока выполняется условие:

```cpp
max_in_iter > correct_eps
```

### Алгоритмическая сложность

- **Одна итерация**: \(O(n^2)\), так как для каждого \(i\) вычисляются суммы по элементам строки матрицы.
- **Общая сложность**: \(O(k \cdot n^2)\), где \(k\) — число итераций до достижения заданной точности \(\varepsilon\).

## 4. Параллельный алгоритм

Параллельный алгоритм использует технологию MPI для распределения вычислений между несколькими процессами. Основная идея заключается в разделении работы по первому измерению (координата i) для каждого шага многошаговой схемы.

### Алгоритм работы

Параллельный алгоритм работает следующим образом:

1. Инициализация MPI окружения:

    - Определяется количество процессов и ранг текущего процесса
    - Все процессы получают одинаковое входное значение

2. Для каждого шага многошаговой схемы (step = 1, 2, 3):

    - Вычисляется общее количество строк для обработки: divisions = n \* step
    - Вычисляется шаг сетки: h = 1.0 / divisions
    - Распределение работы между процессами:

        - Вычисляется количество строк на процесс: rows_per_process = divisions / size
        - Вычисляется остаток от деления: remainder = divisions % size
          Остаток показывает, сколько строк остается после равномерного распределения. Эти дополнительные строки распределяются между первыми процессами
        - Каждый процесс получает свой диапазон строк:
            - start_i = rank \* rows_per_process + (rank < remainder ? rank : remainder)
            - end_i = start_i + rows_per_process + (rank < remainder ? 1 : 0)
              Первые remainder процессов получают на одну строку больше, чем остальные процессы

    - Каждый процесс вычисляет локальный результат для своего диапазона:

        - Для каждой строки i от start_i до end_i:
            - Для всех j от 0 до divisions-1:
                - Для всех k от 0 до divisions-1:
                    - Вычисляются координаты: x = (i + 0.5) _ h, y = (j + 0.5) _ h, z = (k + 0.5) \* h
                    - Вычисляется значение функции: f(x, y, z) = x*y*z + x^2 + y^2 + z^2
                    - Добавляется вклад: local_result += f(x, y, z) \* h^3

    - Сбор результатов:
        - Процессы с rank != 0 отправляют свой local_result процессу с rank = 0 через MPI_Send
        - Процесс с rank = 0 получает результаты от всех остальных процессов через MPI_Recv и суммирует их
        - Процесс с rank = 0 добавляет свой локальный результат к общей сумме

3. После обработки всех трех шагов:
    - Процесс с rank = 0 вычисляет среднее значение: total_result = (sum_step1 + sum_step2 + sum_step3) / 3.0
    - Процесс с rank = 0 округляет результат до целого числа
    - Финальный результат рассылается всем процессам через MPI_Bcast

### Особенности реализации

-   Распределение нагрузки учитывает остаток от деления, чтобы более равномерно распределить работу между процессами
-   Каждый процесс работает независимо над своим диапазоном данных, что минимизирует необходимость синхронизации
-   Коммуникация происходит только после завершения вычислений для каждого шага, что снижает накладные расходы

Алгоритмическая сложность для каждого процесса: O(n^3 / P), где P - количество процессов. Общая сложность остается O(n^3), но время выполнения уменьшается пропорционально количеству процессов

## 5. Схема распараллеливания

Для параллельной реализации используется технология MPI. Распараллеливание выполняется по первому измерению (координата i) для каждого шага многошаговой схемы.

### Распределение данных

Для каждого шага (step = 1, 2, 3):

-   Общее количество строк для обработки: divisions = n \* step
-   Строки распределяются между процессами равномерно с учетом остатка
-   Каждый процесс обрабатывает свой диапазон строк: от start_i до end_i
-   Для каждой строки процесс вычисляет интеграл по всем значениям j и k

### Коммуникации между процессами

1. Каждый процесс вычисляет локальный результат для своего диапазона строк
2. Процессы с rank != 0 отправляют свои результаты процессу с rank = 0 с помощью MPI_Send
3. Процесс с rank = 0 собирает все результаты с помощью MPI_Recv и суммирует их
4. Процесс с rank = 0 вычисляет среднее значение результатов всех трех шагов
5. Финальный результат рассылается всем процессам с помощью MPI_Bcast

### Схема работы

```
Шаг 1: divisions = n * 1
  Процесс 0: строки от 0 до rows_per_process_1
  Процесс 1: строки от rows_per_process_1 до rows_per_process_1 + rows_per_process_2
  ...
  Процесс p-1: строки от ... до divisions

Шаг 2: divisions = n * 2
  Аналогичное распределение

Шаг 3: divisions = n * 3
  Аналогичное распределение

После каждого шага: сбор результатов на процесс 0
После всех шагов: вычисление среднего и рассылка результата
```

## 6. Детали реализации

### Структура кода

Файлы проекта:

-   seq/src/ops_seq.cpp - последовательная реализация
-   mpi/src/ops_mpi.cpp - параллельная реализация с использованием MPI
-   common/include/common.hpp - общие определения типов данных
-   tests/functional/main.cpp - функциональные тесты
-   tests/performance/main.cpp - тесты производительности

### Основные функции

**Последовательная реализация:**

-   IntegrandFunction(x, y, z) - вычисление значения подынтегральной функции
-   ComputeRectangleIntegral(n, step) - вычисление интеграла для одного шага

**Параллельная реализация:**

-   ComputeRectangleIntegralPartial(n, step, start_i, end_i) - вычисление части интеграла для заданного диапазона строк
-   Распределение работы между процессами в RunImpl()
-   Результаты собираются при помощи MPI_Send/MPI_Recv
-   Рассылка финального результата при помощи MPI_Bcast

## 7. Экспериментальная среда

| Компонент  | Значение                                |
| ---------- | --------------------------------------- |
| CPU        | Apple M2 (8 cores)                      |
| RAM        | 16 GB                                   |
| ОС         | OS: Ubuntu 24.04 (DevContainer / Mac)   |
| Компилятор | GCC 13.3.0 (g++), C++20, CMake, Release |
| MPI        | mpirun (Open MPI) 4.1.6                 |

### Тестовые данные

Для тестов производительности используется фиксированное значение входного параметра n = 120

## 8. Анализ

### 7.1 Корректность

Корректность работы проверялась следующими способами:

1. **Функциональные тесты**: Написано 48 функциональных тестов, которые проверяют:

    - Корректность вычислений для различных значений входного параметра (от 1 до 20)
    - Граничные случаи
    - Консистентность результатов при повторных запусках
    - Корректность работы методов Validation, PreProcessing и PostProcessing

2. **Проверка инвариантов**: Проверяется, что результат всегда положителен и находится в разумных пределах.

### 7.2 Производительность

Производительность измерялась на 1, 2, 4, 5, 8, 10, 15 и 20 процессах. Для каждого количества процессов выполнялись два типа измерений:

1. **task_run** - измеряется время выполнения только метода Run()
2. **task_pipeline** - измеряется время выполнения всего пайплайна

Ускорение вычисляется как отношение времени последовательной реализации к времени параллельной реализации:

Speedup = T_seq / T_parallel

Эффективность вычисляется как отношение ускорения к количеству процессов:

Efficiency = Speedup / P, где P - количество процессов

#### Таблица 1: Результаты производительности для task_run

| Процессов | Время, с | Ускорение | Эффективность |
| --------- | -------- | --------- | ------------- |
| seq (1)   | 1.223    | 1.00      | N/A           |
| mpi (1)   | 1.234    | 0.99      | 99.0%         |
| mpi (2)   | 0.631    | 1.94      | 97.0%         |
| mpi (4)   | 0.318    | 3.84      | 96.0%         |
| mpi (5)   | 0.359    | 3.41      | 68.2%         |
| mpi (8)   | 0.344    | 3.56      | 44.5%         |
| mpi (10)  | 0.355    | 3.44      | 34.4%         |
| mpi (15)  | 0.397    | 3.08      | 20.5%         |
| mpi (20)  | 0.345    | 3.54      | 17.7%         |

#### Таблица 2: Результаты производительности для task_pipeline

| Процессов | Время, с | Ускорение | Эффективность |
| --------- | -------- | --------- | ------------- |
| seq (1)   | 1.243    | 1.00      | N/A           |
| mpi (1)   | 1.236    | 1.01      | 101.0%        |
| mpi (2)   | 0.638    | 1.95      | 97.5%         |
| mpi (4)   | 0.318    | 3.91      | 97.8%         |
| mpi (5)   | 0.403    | 3.08      | 61.6%         |
| mpi (8)   | 0.360    | 3.45      | 43.1%         |
| mpi (10)  | 0.417    | 2.98      | 29.8%         |
| mpi (15)  | 0.450    | 2.76      | 18.4%         |
| mpi (20)  | 0.572    | 2.17      | 10.9%         |

Анализ результатов показывает следующее:

1. **Ускорение для task_run**: Максимальное ускорение достигается при 4 процессах (3.84), что демонстрирует отличное масштабирование на небольшом количестве процессов. При 2 процессах ускорение составляет 1.94 с эффективностью 97.0%, что указывает на практически идеальное масштабирование. При 8 процессах ускорение составляет 3.56, но эффективность падает до 44.5%, что указывает на начало снижения эффективности. Максимальное ускорение среди всех конфигураций достигается при 20 процессах (3.54), однако эффективность при этом низкая (17.7%)

2. **Ускорение для task_pipeline**: Максимальное ускорение достигается при 4 процессах (3.91), что является лучшим результатом среди всех конфигураций. При 2 процессах ускорение составляет 1.95 с эффективностью 97.5%, демонстрируя отличное масштабирование. При 8 процессах ускорение снижается до 3.45, а эффективность падает до 43.1%. При 20 процессах ускорение значительно падает (2.17), что указывает на значительные накладные расходы на коммуникацию и синхронизацию.

3. **Эффективность**: Для task_run эффективность очень высокая при малом количестве процессов: 97.0% при 2 процессах и 96.0% при 4 процессах. При 8 процессах эффективность падает до 44.5%, что указывает на начало снижения эффективности масштабирования. Для task_pipeline эффективность также очень высокая при малом количестве процессов: 97.5% при 2 процессах и 97.8% при 4 процессах, что является отличным показателем. При 8 процессах эффективность снижается до 43.1%

4. **Сравнение task_run и task_pipeline**: Task_run показывает лучшие результаты, чем task_pipeline, особенно при большом количестве процессов. Это происходит из-за того, что task_run измеряет только время выполнения основной вычислительной части, в отличие от task_pipeline, который включает накладные расходы на валидацию, предобработку и постобработку.

Основные факторы, влияющие на производительность:

-   Накладные расходы на коммуникацию между процессами увеличиваются с ростом количества процессов
-   Неравномерность распределения нагрузки при большом количестве процессов
-   Время синхронизации процессов
-   Для task_pipeline дополнительное время на выполнение Validation, PreProcessing и PostProcessing методов

## 9. Выводы

В ходе выполнения работы мной были реализованы последовательный и MPI алгоритмы вычисления многомерных интегралов методом прямоугольников с использованием многошаговой схемы.

Основное:

-   Реализована последовательная версия алгоритма
-   Реализована параллельная версия с использованием технологии MPI
-   Функциональные тесты покрывают работу
-   Проведен сравнительный анализ производительности

Возможные ограничения:

-   При большом количестве процессов накладные расходы на коммуникацию могут снижать эффективность
-   Неравномерность распределения нагрузки при некратном делении количества строк на количество процессов

## 10. Источники

1. Сысоев А. В. Курс лекций по параллельному программированию

2. Документация Open MPI https://www.open-mpi.org/doc/

3. Microsoft Функции MPI https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-functions

## Приложение

### Фрагмент кода: вычисление интеграла для одного шага (seq)

```cpp
double ComputeRectangleIntegral(int n, int step) {
  double result = 0.0;
  int divisions = n * step;
  double h = 1.0 / divisions;

  for (int i = 0; i < divisions; i++) {
    for (int j = 0; j < divisions; j++) {
      for (int k = 0; k < divisions; k++) {
        double x = (i + 0.5) * h;
        double y = (j + 0.5) * h;
        double z = (k + 0.5) * h;
        result += IntegrandFunction(x, y, z) * h * h * h;
      }
    }
  }

  return result;
}
```

### Фрагмент кода: распределение работы и сбор результатов (MPI)

```cpp
for (int step = 1; step <= 3; step++) {
  int divisions = GetInput() * step;
  int rows_per_process = divisions / size;
  int remainder = divisions % size;

  int start_i = (rank * rows_per_process) + (rank < remainder ? rank : remainder);
  int end_i = start_i + rows_per_process + (rank < remainder ? 1 : 0);

  double local_result = ComputeRectangleIntegralPartial(GetInput(), step, start_i, end_i);

  if (rank == 0) {
    total_result += local_result;
    for (int proc = 1; proc < size; proc++) {
      double recv_result = 0.0;
      MPI_Recv(&recv_result, 1, MPI_DOUBLE, proc, step, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
      total_result += recv_result;
    }
  } else {
    MPI_Send(&local_result, 1, MPI_DOUBLE, 0, step, MPI_COMM_WORLD);
  }
}
```
